{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9989d622",
   "metadata": {},
   "source": [
    "# H1 Stock Reconciliation Transformation Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719cc304",
   "metadata": {},
   "source": [
    "## H2 Specifications\n",
    "Andrew has provide the intended behaviour here:\n",
    "[Andrews Spec](https://mapofag.sharepoint.com/:w:/s/Project/project_MapofAg_PureFarming/Eb6Rmp1Nk4pBmKmShQm2_RoB4mbEvtc6XRdjKJGynzOESw?e=4UAoe1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a30377",
   "metadata": {},
   "source": [
    "**TimeZone**\n",
    "To start with, we will need to obtain the timezone of the holding that we are transforming.\n",
    "For Easy Dairy, this is passed when the holding upload's the file via Provider.\n",
    "This timezone will then be mapped to determine which hamisphere the holding belongs to. \n",
    "We need this to determine the bithing period of an animal:\n",
    "\n",
    "- **Northern Hamisphere**: 01/01/2025 to 31/12/2025\n",
    "- **Southern Hamisphere**: 01/07/2025 to 30/06/2026\n",
    "\n",
    "As for mapping, after discussing with Chris H and looking at which client/hondings are to be using stock rec, we have decided to only map NZ and Australian timezones for now. The rest would be considered as Northern hamisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347611f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095e4c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of the timezone being fetched \n",
    "print('Select a timezone from the list below: Just type in the number')\n",
    "print('1. Pacific/Auckland')\n",
    "print('2. Australia/Sydney')\n",
    "print('3. America/New_York')\n",
    "tz_choise = input('Enter the number corresponding to your timezone: ')\n",
    "if tz_choise == '1':\n",
    "    timezone = 'Pacific/Auckland'\n",
    "elif tz_choise == '2':\n",
    "    timezone = 'Australia/Sydney'\n",
    "elif tz_choise == '3':\n",
    "    timezone = 'America/New_York'\n",
    "else:\n",
    "    print('Invalid choice')\n",
    "    timezone = tz_choise\n",
    "\n",
    "print (f'You have selected timezone: {timezone}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291108fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Timezone hamisphere mapping\n",
    "\n",
    "def hamisphere_mapper(timezone):\n",
    "    if timezone in ['Australia/Sydney', 'Australia/Melbourne', 'Australia/Brisbane', 'Australia/Adelaide', 'Australia/Perth', 'Australia/Hobart']:\n",
    "        return 'Southern'\n",
    "    elif timezone in ['Pacific/Auckland']:\n",
    "        return 'Southern'\n",
    "    else:\n",
    "        return 'Northern'  \n",
    "\n",
    "tz = hamisphere_mapper(timezone)\n",
    "print(f'The timezone {timezone} is in the {tz} Hemisphere.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a37aa76",
   "metadata": {},
   "source": [
    "# H 2 Fetching the Data\n",
    "Animal and  events are obtained via the Primary Data Store DB.\n",
    "Initial analysis on the data suggests that there are two ways of approaching it. The first would be to query and combine all tables and let the DB do most of the heavy lifting. The other way is to combine the data within the Lambda itself. \n",
    "\n",
    "I have tried both methods and both have their pros and cons as shown in the example below. This is left to the devs to decide the best approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f2ca6",
   "metadata": {},
   "source": [
    "# H 3 Fetching via query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343e8a0",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--sql query\n",
    "WITH animals AS (\n",
    "  SELECT\n",
    "      entity_id,\n",
    "      animal_id,\n",
    "      specie,\n",
    "      gender,\n",
    "      status\n",
    "  FROM data.livestock_animal\n",
    "  WHERE entity_id = 'd574c19f-8a6f-4787-9561-19afaa5ce661'\n",
    "    AND meta_is_deleted = FALSE\n",
    "),\n",
    "events AS (\n",
    "  -- death\n",
    "  SELECT\n",
    "      a.entity_id,\n",
    "      a.specie,\n",
    "      a.gender,\n",
    "      a.status,\n",
    "      a.animal_id,\n",
    "      d.event_date_time,\n",
    "      'death'::text AS event_type\n",
    "  FROM animals a\n",
    "  JOIN data.livestock_animal_death d\n",
    "    ON d.entity_id = a.entity_id\n",
    "   AND d.animal_id = a.animal_id\n",
    "   AND d.meta_is_deleted = FALSE\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  -- birth registration\n",
    "  SELECT\n",
    "      a.entity_id,\n",
    "      a.specie,\n",
    "      a.gender,\n",
    "      a.status,\n",
    "      a.animal_id,\n",
    "      b.event_date_time,\n",
    "      'birth'::text AS event_type\n",
    "  FROM animals a\n",
    "  JOIN data.livestock_animal_birth_registration b\n",
    "    ON b.entity_id = a.entity_id\n",
    "   AND b.animal_id = a.animal_id\n",
    "   AND b.meta_is_deleted = FALSE\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  -- departure\n",
    "  SELECT\n",
    "      a.entity_id,\n",
    "      a.specie,\n",
    "      a.gender,\n",
    "      a.status,\n",
    "      a.animal_id,\n",
    "      dp.event_date_time,\n",
    "      'departure'::text AS event_type\n",
    "  FROM animals a\n",
    "  JOIN data.livestock_animal_departure dp\n",
    "    ON dp.entity_id = a.entity_id\n",
    "   AND dp.animal_id = a.animal_id\n",
    "   AND dp.meta_is_deleted = FALSE\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  -- arrival\n",
    "  SELECT\n",
    "      a.entity_id,\n",
    "      a.specie,\n",
    "      a.gender,\n",
    "      a.status,\n",
    "      a.animal_id,\n",
    "      ar.event_date_time,\n",
    "      'arrival'::text AS event_type\n",
    "  FROM animals a\n",
    "  JOIN data.livestock_animal_arrival ar\n",
    "    ON ar.entity_id = a.entity_id\n",
    "   AND ar.animal_id = a.animal_id\n",
    "   AND ar.meta_is_deleted = FALSE\n",
    ")\n",
    "SELECT\n",
    "  entity_id, specie, gender, status, animal_id, event_date_time, event_type\n",
    "FROM events\n",
    "ORDER BY event_date_time;\n",
    "\n",
    "\n",
    "-- part 2\n",
    "select* from data.livestock_animal la \n",
    "where la.identifier_id = '416075060'\n",
    "and entity_id ='9a66abe5-d071-4326-83e8-e7fd63a686bf'\n",
    "\n",
    "\n",
    "\n",
    "create temp table animal_main AS\n",
    "select entity_id,animal_id, identifier_id, birth_date, status, location_identifier_id, location_identifier_scheme\n",
    "from data.livestock_animal la \n",
    "where \n",
    "la.entity_id ='9a66abe5-d071-4326-83e8-e7fd63a686bf'\n",
    "and la.animal_id is not null\n",
    "and la.meta_is_deleted = false\n",
    "--and la.birth_date <= TIMESTAMPTZ '2023-11-01 00:00:00+00'\n",
    "--and la.birth_date > TIMESTAMPTZ '2022-09-01 00:00:00+00'\n",
    "order by birth_date desc\n",
    "limit 100;\n",
    "\n",
    "\n",
    "\n",
    "select* from data.livestock_animal_arrival \n",
    "--where entity_id ='9a66abe5-d071-4326-83e8-e7fd63a686bf'\n",
    "order by animal_identifier_id desc\n",
    "limit 50;\n",
    "\n",
    "select* from data.livestock_animal_death\n",
    "where entity_id ='9a66abe5-d071-4326-83e8-e7fd63a686bf'\n",
    "and animal_identifier_id in (select identifier_id from animal_main)\n",
    "order by event_date_time  desc\n",
    "\n",
    "\n",
    "drop table if exists animal_main\n",
    "\n",
    "select la.entity_id,la.identifier_id, la.birth_date, jt.event_date_time\n",
    "from data.livestock_animal la \n",
    "join data.livestock_animal_death jt on jt.animal_identifier_id = la.identifier_id \n",
    "and jt.entity_id = la.entity_id\n",
    "where la.entity_id = '9a66abe5-d071-4326-83e8-e7fd63a686bf'\n",
    "order by la.birth_date desc\n",
    "limit 100;\n",
    "\n",
    "select la.entity_id,la.identifier_id, la.birth_date, jt.event_date_time\n",
    "from data.livestock_animal la \n",
    "join data.livestock_animal_birth_registration jt on jt.animal_identifier_id = la.identifier_id \n",
    "and jt.entity_id = la.entity_id\n",
    "where la.entity_id = '9a66abe5-d071-4326-83e8-e7fd63a686bf'\n",
    "order by la.birth_date desc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc2109",
   "metadata": {},
   "source": [
    "# H 3 combing via python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78cf1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H 4 Animal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef323499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# reading the data. This example I am using a local file path.\n",
    "# When reading from PDS this will need to be updated.\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['birth_date'] = pd.to_datetime(df['birth_date'], errors='coerce', utc=True)\n",
    "    df = df[df['meta_is_deleted'] == False]\n",
    "    return df\n",
    "\n",
    "# From the data, I then saperate the birth periods based on hemisphere. Hemisphere is inputed in this case.\n",
    "# We will need to get the timezone of the holding then map it to hemisphere.\n",
    "def get_birth_period(date, hemisphere):\n",
    "    if pd.isnull(date):\n",
    "        return 'Unknown'\n",
    "    year = date.year\n",
    "    if hemisphere == 'southern':\n",
    "        return f\"{year}-{year+1}\" if date.month >= 7 else f\"{year-1}-{year}\"\n",
    "    else:\n",
    "        return str(year)\n",
    "\n",
    "# We then need to group the data by inventory clasifications. Gender, Species and Birth Period.\n",
    "# We then count the number of animals in each group.\n",
    "def process_data(df, hemisphere):\n",
    "    df['birthPeriod'] = df['birth_date'].apply(lambda x: get_birth_period(x, hemisphere))\n",
    "    \n",
    "    grouped = df.groupby(['specie', 'gender', 'birthPeriod']).agg({\n",
    "        'animal_id': lambda x: tuple(x),  # Collect animal_ids as a tuple\n",
    "        'animal_id': 'count'  # Count of animals\n",
    "    }).reset_index().rename(columns={'animal_id': 'count', '<lambda_0>': 'animal_ids'})\n",
    "    \n",
    "    # Fix column names after aggregation\n",
    "    grouped['animal_ids'] = df.groupby(['specie', 'gender', 'birthPeriod'])['animal_id'].apply(tuple).reset_index(drop=True)\n",
    "    \n",
    "    return grouped.to_dict(orient='records')\n",
    "\n",
    "# The output will need to be transformed to the correct json format.\n",
    "\n",
    "def transform_result(data, hemisphere):\n",
    "    transformed = []\n",
    "    for item in data:\n",
    "        new_item = {\n",
    "            \"species\": item[\"specie\"],\n",
    "            \"sex\": item[\"gender\"],\n",
    "            \"count\": item[\"count\"],\n",
    "            \"animal_id\": item[\"animal_ids\"]  # Add the tuple of IDs\n",
    "        }\n",
    "\n",
    "        bp = item[\"birthPeriod\"]\n",
    "        if bp == \"Unknown\":\n",
    "            new_item[\"birthPeriod\"] = \"Unknown\"\n",
    "        elif hemisphere == \"southern\":\n",
    "            start_year, end_year = map(int, bp.split('-'))\n",
    "            new_item[\"birthPeriod\"] = f\"01-07-{start_year}/30-06-{end_year}\"\n",
    "        else:\n",
    "            year = int(bp)\n",
    "            new_item[\"birthPeriod\"] = f\"01-01-{year}/31-12-{year}\"\n",
    "\n",
    "        transformed.append(new_item)\n",
    "    return transformed\n",
    "\n",
    "\n",
    "def main():\n",
    "    file_path = \"../example_files/livestock_animal_202509160441.csv\"\n",
    "    hemisphere = input(\"Enter hemisphere (northern/southern): \").strip().lower()\n",
    "    \n",
    "    df = load_data(file_path)\n",
    "    result = process_data(df, hemisphere)\n",
    "    transformed_result = transform_result(result, hemisphere)\n",
    "\n",
    "    print(json.dumps(transformed_result, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b63583",
   "metadata": {},
   "source": [
    "# H 4 Event files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining the event files\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "FILES = [\n",
    "    \"../example_files/livestock_animal_arrival_202509160444.csv\",\n",
    "    \"../example_files/livestock_animal_birth_registration_202509160442.csv\",\n",
    "    \"../example_files/livestock_animal_death_202509160442.csv\",\n",
    "    \"../example_files/livestock_animal_departure_202509160443.csv\",\n",
    "]\n",
    "\n",
    "REQUIRED_COLS = [\"animal_id\", \"event_date_time\", \"entity_id\", \"meta_is_deleted\"]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def infer_event_from_path(p: Path) -> str:\n",
    "    \"\"\"\n",
    "    Infer event type from filename.\n",
    "    Always map any birth-related file to 'birth'.\n",
    "    \"\"\"\n",
    "    s = p.stem.lower()\n",
    "    if \"departure\" in s:\n",
    "        return \"departure\"\n",
    "    if \"death\" in s:\n",
    "        return \"death\"\n",
    "    if \"arrival\" in s:\n",
    "        return \"arrival\"\n",
    "    if \"birth_registration\" in s or \"birth\" in s:\n",
    "        return \"birth\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def parse_bool_series(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Coerce common truthy/falsey strings to booleans.\n",
    "    Unrecognized values default to False.\n",
    "    \"\"\"\n",
    "    map_values = {\n",
    "        \"true\": True, \"t\": True, \"1\": True, \"yes\": True, \"y\": True,\n",
    "        \"false\": False, \"f\": False, \"0\": False, \"no\": False, \"n\": False, \"\": False\n",
    "    }\n",
    "    return (\n",
    "        series.astype(str)\n",
    "              .str.strip()\n",
    "              .str.lower()\n",
    "              .map(map_values)\n",
    "              .fillna(False)\n",
    "              .astype(bool)\n",
    "    )\n",
    "\n",
    "\n",
    "def read_and_tag(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read one CSV, keep required columns, standardize types, and add 'event'.\n",
    "    \"\"\"\n",
    "    p = Path(file_path)\n",
    "    event = infer_event_from_path(p)\n",
    "\n",
    "    # Prefer reading only needed columns\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            p,\n",
    "            usecols=REQUIRED_COLS,\n",
    "            dtype={\"animal_id\": \"string\", \"entity_id\": \"string\"},\n",
    "            low_memory=False,\n",
    "        )\n",
    "    except ValueError:\n",
    "        # Fallback if usecols fails (or columns out of order)\n",
    "        df_all = pd.read_csv(p, low_memory=False)\n",
    "        missing = [c for c in REQUIRED_COLS if c not in df_all.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"{p.name} is missing required columns: {missing}\")\n",
    "        df = df_all[REQUIRED_COLS].copy()\n",
    "        df[\"animal_id\"] = df[\"animal_id\"].astype(\"string\")\n",
    "        df[\"entity_id\"] = df[\"entity_id\"].astype(\"string\")\n",
    "\n",
    "    # Normalize\n",
    "    df[\"meta_is_deleted\"] = parse_bool_series(df[\"meta_is_deleted\"])\n",
    "    df[\"event_date_time\"] = pd.to_datetime(df[\"event_date_time\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    # Add event\n",
    "    df[\"event\"] = event\n",
    "\n",
    "    # Final column order\n",
    "    return df[[\"animal_id\", \"event_date_time\", \"entity_id\", \"meta_is_deleted\", \"event\"]]\n",
    "\n",
    "\n",
    "def get_combined_records(files: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Combine all files and return a list of dicts.\n",
    "    No files are writtenâ€”purely in-memory.\n",
    "    \"\"\"\n",
    "    frames = [read_and_tag(f) for f in files]\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # Optional: dedupe + sort for determinism\n",
    "    combined = (\n",
    "        combined.drop_duplicates(subset=[\"animal_id\", \"event_date_time\", \"event\", \"entity_id\"])\n",
    "                .sort_values([\"animal_id\", \"event_date_time\"], kind=\"stable\")\n",
    "                .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Serialize datetime to ISO 8601 without timezone.\n",
    "    # NaT becomes None so JSON shows null (not \"NaT\").\n",
    "    iso = combined[\"event_date_time\"].dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    combined[\"event_date_time\"] = iso.where(pd.notna(iso), None)\n",
    "\n",
    "    # Produce list[dict]\n",
    "    records: List[Dict] = combined.to_dict(orient=\"records\")\n",
    "    return records\n",
    "\n",
    "\n",
    "# Example usage (won't save files)\n",
    "if __name__ == \"__main__\":\n",
    "    records = get_combined_records(FILES)\n",
    "    # Quick peek\n",
    "    print(f\"Total records: {len(records)}\")\n",
    "    # Sample 3 rows\n",
    "    for r in records[:3]:\n",
    "        print(r)\n",
    "    # Optional: event counts\n",
    "    from collections import Counter\n",
    "    print(\"Counts by event:\", dict(Counter(r[\"event\"] for r in records)))\n",
    "\n",
    "\n",
    "   # \"/home/wanmusa/work/stock_recon/example_data/livestock_animal_arrival_202509160444.csv\"\n",
    "    #\"/home/wanmusa/work/stock_recon/example_data/livestock_animal_birth_registration_202509160442.csv\"\n",
    "    #\"/home/wanmusa/work/stock_recon/example_data/livestock_animal_death_202509160442.csv\"\n",
    "    #\"/home/wanmusa/work/stock_recon/example_data/livestock_animal_departure_202509160443.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f81b4",
   "metadata": {},
   "source": [
    "# H 4 Aggrigate both processes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a3690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inventory_aggregator.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import calendar\n",
    "from datetime import datetime, timezone\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "# Keep your existing modules unmodified\n",
    "import stock_recon_mapping\n",
    "import event_file_comb\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helpers: IDs, dates, formatting\n",
    "# -------------------------\n",
    "\n",
    "def _to_str_id(x: Any) -> str:\n",
    "    return str(x).strip()\n",
    "\n",
    "def _parse_event_dt(s: str) -> Optional[datetime]:\n",
    "    \"\"\"\n",
    "    event_file_comb emits naive 'YYYY-MM-DDTHH:MM:SS'. Treat as UTC.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.strptime(s.strip(), \"%Y-%m-%dT%H:%M:%S\").replace(tzinfo=timezone.utc)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _month_bounds_utc(year: int, month: int) -> Tuple[datetime, datetime]:\n",
    "    \"\"\"\n",
    "    Calendar month bounds as UTC midnights:\n",
    "      - start: first day 00:00:00Z\n",
    "      - end: last day 00:00:00Z (matches your example end stamp)\n",
    "    \"\"\"\n",
    "    start = datetime(year, month, 1, tzinfo=timezone.utc)\n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    end = datetime(year, month, last_day, tzinfo=timezone.utc)\n",
    "    return start, end\n",
    "\n",
    "def _format_z(dt: datetime) -> str:\n",
    "    # 'YYYY-MM-DDTHH:MM:SS.000Z' at UTC midnight per your example\n",
    "    return dt.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "\n",
    "def _month_iter_inclusive(start: datetime, end: datetime) -> List[Tuple[datetime, datetime]]:\n",
    "    \"\"\"\n",
    "    Build inclusive list of (start,end) month boundaries. Only year-month is used.\n",
    "    \"\"\"\n",
    "    s = datetime(start.year, start.month, 1, tzinfo=timezone.utc)\n",
    "    e = datetime(end.year, end.month, 1, tzinfo=timezone.utc)\n",
    "    out = []\n",
    "    y, m = s.year, s.month\n",
    "    while (y < e.year) or (y == e.year and m <= e.month):\n",
    "        out.append(_month_bounds_utc(y, m))\n",
    "        if m == 12:\n",
    "            y, m = y + 1, 1\n",
    "        else:\n",
    "            m += 1\n",
    "    return out\n",
    "\n",
    "def _normalize_birth_period(bp: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Best-effort normalize to 'YYYY-MM-DD/YYYY-MM-DD'. 'Unknown' stays 'Unknown'.\n",
    "    Accepts 'DD-MM-YYYY/DD-MM-YYYY' from your transformer, or already ISO strings.\n",
    "    \"\"\"\n",
    "    if not bp or str(bp).lower() == \"unknown\":\n",
    "        return \"Unknown\"\n",
    "    if \"/\" not in bp:\n",
    "        return bp\n",
    "\n",
    "    left, right = bp.split(\"/\", 1)\n",
    "\n",
    "    def parse_any(x: str) -> datetime:\n",
    "        x = x.strip()\n",
    "        for fmt in (\"%d-%m-%Y\", \"%Y-%m-%d\"):\n",
    "            try:\n",
    "                return datetime.strptime(x, fmt)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        raise ValueError\n",
    "\n",
    "    try:\n",
    "        ldt, rdt = parse_any(left), parse_any(right)\n",
    "        return f\"{ldt.strftime('%Y-%m-%d')}/{rdt.strftime('%Y-%m-%d')}\"\n",
    "    except Exception:\n",
    "        return bp\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main aggregation\n",
    "# -------------------------\n",
    "\n",
    "def aggregate_inventory_rollup(\n",
    "    *,\n",
    "    inventory_csv_path: str,\n",
    "    hemisphere: str = \"southern\",\n",
    "    event_files: Optional[List[str]] = None,  # None -> event_file_comb.FILES\n",
    "    duration: str = \"M\",\n",
    "    open_value: int = 0,             # per your requirement\n",
    "    force_close: Optional[int] = 0,   # default close = 0; set to None to compute from events\n",
    "    output_json_path: str = \"inventory_rollup.json\",\n",
    "    include_unclassified: bool = False\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build monthly roll-ups per stock class across the full month range\n",
    "    present in the dataset (first -> last month).\n",
    "\n",
    "    Stock class key: (species, sex, birthPeriod)\n",
    "    'open' and 'close' default to 0. Counts are computed from events.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- 1) Inventory classes from stock_recon_mapping -----\n",
    "    df = stock_recon_mapping.load_data(inventory_csv_path)\n",
    "    grouped = stock_recon_mapping.process_data(df, hemisphere)\n",
    "    classes = stock_recon_mapping.transform_result(grouped, hemisphere)\n",
    "\n",
    "    # Normalize classes for matching\n",
    "    norm_classes = []\n",
    "    for ic in classes:\n",
    "        norm_classes.append({\n",
    "            \"species\": str(ic.get(\"species\", \"\")).strip(),\n",
    "            \"sex\": str(ic.get(\"sex\", \"\")).strip(),\n",
    "            \"birthPeriod\": _normalize_birth_period(ic.get(\"birthPeriod\")),\n",
    "            \"animal_ids\": {_to_str_id(aid) for aid in (ic.get(\"animal_id\") or [])},\n",
    "        })\n",
    "\n",
    "    # Build an index: animal_id -> class index (first match wins)\n",
    "    animal_to_class_idx: Dict[str, int] = {}\n",
    "    for idx, ic in enumerate(norm_classes):\n",
    "        for aid in ic[\"animal_ids\"]:\n",
    "            if aid not in animal_to_class_idx:\n",
    "                animal_to_class_idx[aid] = idx\n",
    "\n",
    "    # ----- 2) Events from event_file_comb -----\n",
    "    files_to_read = event_files if event_files is not None else getattr(event_file_comb, \"FILES\", [])\n",
    "    events = event_file_comb.get_combined_records(files_to_read)\n",
    "\n",
    "    normalized_events = []\n",
    "    for e in events:\n",
    "        if e.get(\"meta_is_deleted\", False):\n",
    "            continue\n",
    "        dt = _parse_event_dt(e.get(\"event_date_time\") or \"\")\n",
    "        if not dt:\n",
    "            continue\n",
    "        aid = _to_str_id(e.get(\"animal_id\", \"\"))\n",
    "        evt = str(e.get(\"event\", \"\")).strip().lower()\n",
    "        normalized_events.append({\"animal_id\": aid, \"event\": evt, \"dt\": dt})\n",
    "\n",
    "    # Optionally include an \"unclassified\" bucket for ids not in classes\n",
    "    unclassified_key = {\"species\": \"Unknown\", \"sex\": \"Unknown\", \"birthPeriod\": \"Unknown\"}\n",
    "    if include_unclassified:\n",
    "        norm_classes.append({**unclassified_key, \"animal_ids\": set()})\n",
    "        unclassified_idx = len(norm_classes) - 1\n",
    "    else:\n",
    "        unclassified_idx = None\n",
    "\n",
    "    class_events: List[Tuple[int, datetime, str]] = []  # (class_idx, dt, event)\n",
    "    for ev in normalized_events:\n",
    "        idx = animal_to_class_idx.get(ev[\"animal_id\"], unclassified_idx)\n",
    "        if idx is None:\n",
    "            continue\n",
    "        class_events.append((idx, ev[\"dt\"], ev[\"event\"]))\n",
    "\n",
    "    # ----- 3) Determine month range from data (events first; fallback to inventory) -----\n",
    "    if class_events:\n",
    "        min_dt = min(x[1] for x in class_events)\n",
    "        max_dt = max(x[1] for x in class_events)\n",
    "    else:\n",
    "        if \"birth_date\" in df and df[\"birth_date\"].notna().any():\n",
    "            min_dt = df[\"birth_date\"].min().to_pydatetime()\n",
    "            max_dt = df[\"birth_date\"].max().to_pydatetime()\n",
    "        else:\n",
    "            today = datetime.now(timezone.utc)\n",
    "            min_dt = max_dt = today\n",
    "\n",
    "    months = _month_iter_inclusive(min_dt, max_dt)\n",
    "\n",
    "    # Pre-bucket event counts per (class_idx, month_index)\n",
    "    per_bucket: Dict[Tuple[int, int], Dict[str, int]] = {}\n",
    "\n",
    "    def month_index_for(dt: datetime) -> int:\n",
    "        y, m = dt.year, dt.month\n",
    "        for i, (ms, _) in enumerate(months):\n",
    "            if ms.year == y and ms.month == m:\n",
    "                return i\n",
    "        return -1\n",
    "\n",
    "    for class_idx, dt, evt in class_events:\n",
    "        mi = month_index_for(dt)\n",
    "        if mi == -1:\n",
    "            continue\n",
    "        key = (class_idx, mi)\n",
    "        if key not in per_bucket:\n",
    "            per_bucket[key] = {\"birth\": 0, \"death\": 0, \"arrival\": 0, \"departure\": 0}\n",
    "        if evt in per_bucket[key]:\n",
    "            per_bucket[key][evt] += 1\n",
    "\n",
    "    # ----- 4) Emit monthly records for EVERY (class, month) -----\n",
    "    out: List[Dict[str, Any]] = []\n",
    "\n",
    "    for mi, (m_start, m_end) in enumerate(months):\n",
    "        for class_idx, ic in enumerate(norm_classes):\n",
    "            counts = per_bucket.get((class_idx, mi), {\"birth\": 0, \"death\": 0, \"arrival\": 0, \"departure\": 0})\n",
    "            births = counts[\"birth\"]\n",
    "            deaths = counts[\"death\"]\n",
    "            arrivals = counts[\"arrival\"]\n",
    "            departures = counts[\"departure\"]\n",
    "\n",
    "            # Default close to 0; set force_close=None to compute from events\n",
    "            computed_close = open_value + births + arrivals - deaths - departures\n",
    "            close_val = force_close if force_close is not None else computed_close\n",
    "\n",
    "            record = {\n",
    "                \"startDate\": _format_z(m_start),\n",
    "                \"endDate\": _format_z(m_end),\n",
    "                \"duration\": duration,\n",
    "                \"open\": open_value,\n",
    "                \"births\": births,\n",
    "                \"deaths\": deaths,\n",
    "                \"close\": close_val,\n",
    "                \"departuresTotal\": departures,\n",
    "                \"arrivalsTotal\": arrivals,\n",
    "                \"inventoryClassification\": {\n",
    "                    \"sex\": ic[\"sex\"],\n",
    "                    \"species\": ic[\"species\"],\n",
    "                    \"name\": f\"{ic['sex']}-{ic['species']}-{m_start.year}\",  # <== your requested format\n",
    "                    \"birthPeriod\": ic[\"birthPeriod\"],\n",
    "                },\n",
    "            }\n",
    "            out.append(record)\n",
    "\n",
    "    # ----- 5) Write JSON and return list of dicts -----\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CLI example\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    results = aggregate_inventory_rollup(\n",
    "        inventory_csv_path=\"/home/wanmusa/work/stock_recon/example_data/livestock_animal_202509160441.csv\",\n",
    "        hemisphere=\"southern\",\n",
    "        event_files=None,      # None -> use event_file_comb.FILES\n",
    "        duration=\"M\",\n",
    "        open_value=0,          # required default\n",
    "        force_close=0,         # required default (set to None to compute)\n",
    "        output_json_path=\"inventory_rollup.json\",\n",
    "        include_unclassified=False\n",
    "    )\n",
    "    print(f\"Wrote {len(results)} records to inventory_rollup.json\")\n",
    "    print(json.dumps(results[:3], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91096b1a",
   "metadata": {},
   "source": [
    "These are just examples. I leave it to the devs to work out the solution. This is an example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
